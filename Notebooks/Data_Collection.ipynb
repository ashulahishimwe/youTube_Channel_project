{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e8c6133-3f8f-4e55-be9b-ae47d3508bf3",
   "metadata": {},
   "source": [
    "# Youtube Channel Insights - Exploratory and Descriptive of Data Collection\n",
    "\n",
    "## Project Goal Summary\n",
    "We’re analyzing a tech YouTube channel’s videos and comments using **YouTube Data API v3** to:\n",
    "\n",
    "- Collect and clean data\n",
    "\n",
    "- Analyze publishing and engagement trends\n",
    "\n",
    "- Do basic sentiment analysis\n",
    "\n",
    "- Visualize everything\n",
    "\n",
    "- Give final insights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3721fe4-ddd1-49f6-bb32-93ab63ead8c4",
   "metadata": {},
   "source": [
    "## Setup API Access\n",
    "Let’s start here.\n",
    "\n",
    "1. Create a Google Cloud Project: Go to https://console.cloud.google.com\n",
    "2. Enable YouTube Data API v3\n",
    "3. Get an API Key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef05501-1649-40e0-b142-63feee462be3",
   "metadata": {},
   "source": [
    "## Get Channel ID\n",
    "We’ll use the API to find Lex Fridman’s `channelId`.\n",
    "\n",
    "- `from googleapiclient.discovery import build`:  To connect to YouTube Data API using Python.\n",
    "\n",
    "- `from dotenv import load_dotenv`:  To securely load the API key from a `.env` file.\n",
    "\n",
    "- `import os` + `os.getenv(\"API_KEY\")`:  \n",
    "  To access the API key stored in environment variables (not hard-coded).\n",
    "\n",
    "This keeps your API key private and secure while allowing authorized access to YouTube data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ee2d4ef-51f7-4bd6-855a-de17dc740453",
   "metadata": {},
   "outputs": [],
   "source": [
    "from googleapiclient.discovery import build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8785fe0-3359-4123-8ef5-fe15a62447f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0598416-08a8-40aa-8e2e-7c10f298b96a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bfd7a370-9c94-40be-ab0f-8893a2d0daab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Channel ID: UCSHZKyawb77ixDdsGog4iWA\n"
     ]
    }
   ],
   "source": [
    "# Create a YouTube API client\n",
    "youtube = build('youtube', 'v3', developerKey=api_key)\n",
    "\n",
    "# Search for the channel by name\n",
    "request = youtube.search().list(\n",
    "    q='Lex Fridman',\n",
    "    type='channel',\n",
    "    part='snippet',\n",
    "    maxResults=1\n",
    ")\n",
    "response = request.execute()\n",
    "\n",
    "# Get the channel ID\n",
    "channel_id = response['items'][0]['snippet']['channelId']\n",
    "print(\"Channel ID:\", channel_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4192c296-1600-461f-a049-20a06edef0c6",
   "metadata": {},
   "source": [
    "## Fetch All Videos From Past 2 Years\n",
    "Now let’s get all video metadata (video ID, title, views, likes, comments, etc.) published in the last 2 years."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c7314af-d535-481b-82d4-ad5173da07b5",
   "metadata": {},
   "source": [
    "### Import Required Libraries\n",
    "\n",
    "- `import pandas as pd`: To store, clean, and analyze tabular data (like CSV files) efficiently.\n",
    "\n",
    "- `import time`: To add delays (`time.sleep()`) between API requests and avoid rate limits.\n",
    "\n",
    "- `from datetime import datetime, timedelta, timezone`: To work with video dates — like filtering videos from the past 2 years.\n",
    "\n",
    "- `import os`: To access environment variables (like your API key) stored on your system.\n",
    "\n",
    "Together, these libraries help collect and prepare YouTube data in a clean, safe, and efficient way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bf41ddaf-7ce2-4d3f-8c55-87a640c36c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import time\n",
    "from datetime import datetime, timedelta, timezone\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3632cf26-6b13-4fa3-b3a3-6ebc032526f4",
   "metadata": {},
   "source": [
    "### Define and Create Directory Paths\n",
    "\n",
    "To ensure reproducibility and organized storage, we programmatically create directories if they don't already exist for:\n",
    "\n",
    "- **raw data**\n",
    "- **processed data**\n",
    "- **results**\n",
    "- **documentation**\n",
    "\n",
    "These directories will store intermediate and final outputs for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a416b7de-c3fe-43f8-b09d-4795c6df09b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get working directory\n",
    "current_dir = os.getcwd()\n",
    "#go one directory up to root directory\n",
    "project_root_dir = os.path.dirname(current_dir)\n",
    "#Define path to data files\n",
    "data_dir = os.path.join(project_root_dir, 'data')\n",
    "raw_dir = os.path.join(data_dir, 'raw')\n",
    "processed_dir = os.path.join(data_dir, 'processed')\n",
    "#Define path to results folder\n",
    "results_dir = os.path.join(project_root_dir, 'results')\n",
    "#Define path to results folder\n",
    "docs_dir = os.path.join(project_root_dir, 'docs')\n",
    "\n",
    "#Create directories if they do not exist\n",
    "os.makedirs(raw_dir, exist_ok=True)\n",
    "os.makedirs(processed_dir, exist_ok=True)\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "os.makedirs(docs_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f13e6a5b-ecee-40f2-bda1-e9f24a628e5b",
   "metadata": {},
   "source": [
    "### Set Up Time Range (Last 2 Years) \n",
    "\n",
    "In this step, we connect to the YouTube Data API to retrieve metadata for all videos posted by the channel in the past 2 years.  \n",
    "We collect useful details, such as `videoId`, `title`, `publishedAt`, `viewCount`, `likeCount`, `commentCount`, `tags`, and `description`.\n",
    "\n",
    "This metadata will be the foundation for our analysis and visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9f61f657-7e3a-492f-a6ff-d5b9da8d5beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Today's date and 2 years back\n",
    "today = datetime.now(timezone.utc).isoformat()\n",
    "two_years_ago = (datetime.now(timezone.utc) - timedelta(days=730)).isoformat()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc92289-1756-4353-9208-a221ae6a1727",
   "metadata": {},
   "source": [
    "## Fetch Videos Metadata\n",
    "\n",
    "### Collect Video Metadata (with Pagination)\n",
    "\n",
    "We use the YouTube Data API to collect metadata for all videos from the specified channel (Lex Fridman) posted in the last two years.\n",
    "\n",
    "This loop handles:\n",
    "- Sending paginated requests using `nextPageToken`\n",
    "- Extracting video `id`, `title`, and `publishedAt` from the search results\n",
    "- Fetching additional video details (like `viewCount`, `likeCount`, `commentCount`, `tags`, and `description`) from the `videos().list()` endpoint\n",
    "\n",
    "The collected data is stored in a list of dictionaries (`video_data`), one for each video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2b0c0d1f-9910-41fc-90ec-6321d204efa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_data = []\n",
    "\n",
    "# Paginate through all videos\n",
    "next_page_token = None\n",
    "\n",
    "while True:\n",
    "    request = youtube.search().list(\n",
    "        part = \"snippet\",\n",
    "        channelId = 'UCSHZKyawb77ixDdsGog4iWA',\n",
    "        maxResults = 50,\n",
    "        publishedAfter = two_years_ago,\n",
    "        publishedBefore = today,\n",
    "        order = \"date\",\n",
    "        type = \"video\",\n",
    "        pageToken = next_page_token\n",
    "    )\n",
    "    response = request.execute()\n",
    "    \n",
    "    for item in response['items']:\n",
    "        video_id = item['id']['videoId']\n",
    "        title = item['snippet']['title']\n",
    "        published = item['snippet']['publishedAt']\n",
    "        \n",
    "        # Get video stats\n",
    "        video_request = youtube.videos().list(\n",
    "            part = \"statistics,snippet\",\n",
    "            id = video_id\n",
    "        )\n",
    "        video_response = video_request.execute()\n",
    "        \n",
    "        for v in video_response['items']:\n",
    "            stats = v['statistics']\n",
    "            snippet = v['snippet']\n",
    "            video_data.append({\n",
    "                \"videoId\": video_id,\n",
    "                \"title\": title,\n",
    "                \"publishedAt\": published,\n",
    "                \"viewCount\": int(stats.get(\"viewCount\", 0)),\n",
    "                \"likeCount\": int(stats.get(\"likeCount\", 0)),\n",
    "                \"commentCount\": int(stats.get(\"commentCount\", 0)),\n",
    "                \"tags\": snippet.get(\"tags\", []),\n",
    "                \"description\": snippet.get(\"description\", \"\")\n",
    "            })\n",
    "\n",
    "    # Check if more pages exist\n",
    "    next_page_token = response.get(\"nextPageToken\")\n",
    "    if not next_page_token:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5637a339-2233-4995-8e83-514e90336c75",
   "metadata": {},
   "source": [
    "## Save Video Metadata to CSV\n",
    "\n",
    "We convert the collected `video_data` list into a pandas DataFrame (`video_df`) and save it as a CSV file.\n",
    "\n",
    "- `os.path.join(...)` creates a system-safe file path  \n",
    "- `to_csv(..., index=False)` writes the data to disk without row numbers  \n",
    "- This file will be used later for data cleaning and analysis\n",
    "\n",
    "The raw video metadata is now safely stored in the `raw_dir` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "95020b38-e1f9-455d-8401-c770fa77bbaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved video metadata. Total videos: 94\n",
      "\n",
      "video datase saved to: C:\\Users\\user\\Documents\\tekHer\\YouTube-Channel-Insights\\data\\raw\\lex_fridman_videos.csv\n"
     ]
    }
   ],
   "source": [
    "video_df = pd.DataFrame(video_data)\n",
    "video_filename = os.path.join(raw_dir, \"lex_fridman_videos.csv\")\n",
    "video_df.to_csv(video_filename, index=False)\n",
    "print(\"Saved video metadata. Total videos:\", len(video_df))\n",
    "print(f\"\\nvideo datase saved to: {video_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e93016-7b9f-4e7c-ad15-9adbe5040949",
   "metadata": {},
   "source": [
    "## Fetch Top 50 Comments for Each Video\n",
    "\n",
    "We use the YouTube Data API to fetch the top 50 top-level comments for each video:\n",
    "\n",
    "- Loop through every `videoId` from the previously collected video data  \n",
    "- Use `commentThreads().list()` to request plain text comments  \n",
    "- Extract key details:\n",
    "  - `authorDisplayName`\n",
    "  - `textDisplay` (original comment)\n",
    "  - `likeCount` on the comment\n",
    "  - `publishedAt` (when the comment was posted)\n",
    "\n",
    "A `time.sleep(1)` delay is added between requests to respect API rate limits.\n",
    "\n",
    "All collected comments are stored in a list of dictionaries (`comment_data`) for later saving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1e24662f-96e9-4922-9041-fc2c096fbb51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "comment_data = []\n",
    "\n",
    "for video_id in video_df['videoId']:\n",
    "    try:\n",
    "        request = youtube.commentThreads().list(\n",
    "            part = \"snippet\",\n",
    "            videoId = video_id,\n",
    "            maxResults = 50,\n",
    "            textFormat = \"plainText\"\n",
    "        )\n",
    "        response = request.execute()\n",
    "\n",
    "        for item in response.get('items', []):\n",
    "            snippet = item['snippet']['topLevelComment']['snippet']\n",
    "            comment_data.append({\n",
    "                \"videoId\": video_id,\n",
    "                \"authorDisplayName\": snippet.get(\"authorDisplayName\", \"\"),\n",
    "                \"textDisplay\": snippet.get(\"textDisplay\", \"\"),\n",
    "                \"likeCount\": snippet.get(\"likeCount\", 0),\n",
    "                \"publishedAt\": snippet.get(\"publishedAt\", \"\")\n",
    "            })\n",
    "\n",
    "        # Delay to avoid hitting rate limits\n",
    "        time.sleep(1)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching comments for {video_id}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b0c47b7-177c-4d44-9321-d7a0b328b4ca",
   "metadata": {},
   "source": [
    "### Save Comments to CSV\n",
    "\n",
    "We convert the collected `comment_data` into a pandas DataFrame (`comments_df`) and save it as a CSV file.\n",
    "\n",
    "- `os.path.join(...)` builds the full file path safely  \n",
    "- `to_csv(..., index=False)` writes the comments to disk  \n",
    "- The saved file will be used in the data cleaning and sentiment analysis steps\n",
    "\n",
    "All YouTube comments are now stored in `lex_fridman_comments.csv` inside the `raw_dir` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e9c6c945-5faf-4fba-bd65-fcb556bab9bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved comments. Total: 4684\n",
      "\n",
      "video datase saved to: C:\\Users\\user\\Documents\\tekHer\\YouTube-Channel-Insights\\data\\raw\\lex_fridman_comments.csv\n"
     ]
    }
   ],
   "source": [
    "comments_df = pd.DataFrame(comment_data)\n",
    "comments_filename = os.path.join(raw_dir, \"lex_fridman_comments.csv\")\n",
    "comments_df.to_csv(comments_filename, index=False)\n",
    "print(\"Saved comments. Total:\", len(comments_df))\n",
    "print(f\"\\nvideo datase saved to: {comments_filename}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
